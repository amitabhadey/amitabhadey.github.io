---
title: "Landmark Papers"
layout: default
---

### Attention Is All You Need 
Vaswani et al. 2017\
This paper introduced the Transformer architecture, which eliminated recurrence and convolution in sequence modeling. It revolutionized NLP by enabling highly parallelized training and improved long-term dependency handling.\
[Read Paper](https://arxiv.org/pdf/1706.03762)

### BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
Devlin et al. 2019\
BERT (Bidirectional Encoder Representations from Transformers) proposed a pre-training approach using masked language modeling (MLM) and next-sentence prediction (NSP). It set state-of-the-art results across multiple NLP benchmarks.\
[Read Paper](https://arxiv.org/pdf/1810.04805)

### RoBERTa: A Robustly Optimized BERT Pretraining Approach
Liu et al. 2019\
RoBERTa built on BERT by removing NSP, training on larger datasets, and using longer sequences. It outperformed BERT in several NLP tasks.\
[Read Paper](https://arxiv.org/pdf/1907.11692)

### Language Models Are Few-Shot Learners
Brown et al. 2020\
This paper introduced GPT-3, a model with 175 billion parameters. GPT-3 demonstrated remarkable few-shot and zero-shot learning capabilities, showcasing the potential of scaling up LLMs.\
[Read Paper](https://arxiv.org/pdf/2005.14165)

### T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
Raffel et al. 2020\
T5 unified NLP tasks into a text-to-text framework, simplifying task formulation. It showed the effectiveness of fine-tuning pre-trained models for various applications.\
[Read Paper](https://arxiv.org/pdf/1910.10683)

### Scaling Laws for Neural Language Models
Kaplan et al. 2020\
This paper uncovered scaling laws for LLMs, showing how model performance improves predictably with increased parameters, data, and compute.\
[Read Paper](https://arxiv.org/pdf/2001.08361)

### PaLM: Scaling Language Modeling with Pathways
Chowdhery et al. 2022\
PaLM scaled language models to 540 billion parameters and demonstrated state-of-the-art performance in reasoning tasks and understanding complex prompts.\
[Read Paper](https://arxiv.org/pdf/2204.02311)

### BLOOM: BigScience Large Open-Science Open-Access Multilingual Language Model
BigScience Workshop 2022\
BLOOM is an open-access, multilingual LLM trained collaboratively by researchers worldwide. It emphasizes inclusivity and transparency in LLM development.\
[Read Paper](https://arxiv.org/pdf/2211.05100)

### Training Compute-Optimal Large Language Models
Hoffmann et al. 2022\
The authors propose the "Chinchilla" strategy, advocating for optimal allocation of compute resources to balance model size and training data for better performance.\
[Read Paper](https://arxiv.org/pdf/2203.15556)

### InstructGPT: Training Language Models to Follow Instructions
Ouyang et al. 2022\
InstructGPT fine-tuned GPT-3 using human feedback to improve alignment with user intentions, marking a significant step towards safer and more user-friendly LLMs.\
[Read Paper](https://arxiv.org/pdf/2203.02155)

