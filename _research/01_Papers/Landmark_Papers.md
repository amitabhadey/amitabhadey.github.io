---
title: "Landmark Papers"
layout: default
---

<h3 id="attention-is-all-you-need">Attention Is All You Need</h3>
<p>Vaswani et al. 2017<br />
This paper introduced the Transformer architecture, which eliminated
recurrence and convolution in sequence modeling. It revolutionized NLP
by enabling highly parallelized training and improved long-term
dependency handling.<br />
<a href="https://arxiv.org/pdf/1706.03762">Read Paper</a></p>
<h3
id="bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding">BERT:
Pre-training of Deep Bidirectional Transformers for Language
Understanding</h3>
<p>Devlin et al. 2019<br />
BERT (Bidirectional Encoder Representations from Transformers) proposed
a pre-training approach using masked language modeling (MLM) and
next-sentence prediction (NSP). It set state-of-the-art results across
multiple NLP benchmarks.<br />
<a href="https://arxiv.org/pdf/1810.04805">Read Paper</a></p>
<h3 id="roberta-a-robustly-optimized-bert-pretraining-approach">RoBERTa:
A Robustly Optimized BERT Pretraining Approach</h3>
<p>Liu et al. 2019<br />
RoBERTa built on BERT by removing NSP, training on larger datasets, and
using longer sequences. It outperformed BERT in several NLP tasks.<br />
<a href="https://arxiv.org/pdf/1907.11692">Read Paper</a></p>
<h3 id="language-models-are-few-shot-learners">Language Models Are
Few-Shot Learners</h3>
<p>Brown et al. 2020<br />
This paper introduced GPT-3, a model with 175 billion parameters. GPT-3
demonstrated remarkable few-shot and zero-shot learning capabilities,
showcasing the potential of scaling up LLMs.<br />
<a href="https://arxiv.org/pdf/2005.14165">Read Paper</a></p>
<h3
id="t5-exploring-the-limits-of-transfer-learning-with-a-unified-text-to-text-transformer">T5:
Exploring the Limits of Transfer Learning with a Unified Text-to-Text
Transformer</h3>
<p>Raffel et al. 2020<br />
T5 unified NLP tasks into a text-to-text framework, simplifying task
formulation. It showed the effectiveness of fine-tuning pre-trained
models for various applications.<br />
<a href="https://arxiv.org/pdf/1910.10683">Read Paper</a></p>
<h3 id="scaling-laws-for-neural-language-models">Scaling Laws for Neural
Language Models</h3>
<p>Kaplan et al. 2020<br />
This paper uncovered scaling laws for LLMs, showing how model
performance improves predictably with increased parameters, data, and
compute.<br />
<a href="https://arxiv.org/pdf/2001.08361">Read Paper</a></p>
<h3 id="palm-scaling-language-modeling-with-pathways">PaLM: Scaling
Language Modeling with Pathways</h3>
<p>Chowdhery et al. 2022<br />
PaLM scaled language models to 540 billion parameters and demonstrated
state-of-the-art performance in reasoning tasks and understanding
complex prompts.<br />
<a href="https://arxiv.org/pdf/2204.02311">Read Paper</a></p>
<h3
id="bloom-bigscience-large-open-science-open-access-multilingual-language-model">BLOOM:
BigScience Large Open-Science Open-Access Multilingual Language
Model</h3>
<p>BigScience Workshop 2022<br />
BLOOM is an open-access, multilingual LLM trained collaboratively by
researchers worldwide. It emphasizes inclusivity and transparency in LLM
development.<br />
<a href="https://arxiv.org/pdf/2211.05100">Read Paper</a></p>
<h3 id="training-compute-optimal-large-language-models">Training
Compute-Optimal Large Language Models</h3>
<p>Hoffmann et al. 2022<br />
The authors propose the “Chinchilla” strategy, advocating for optimal
allocation of compute resources to balance model size and training data
for better performance.<br />
<a href="https://arxiv.org/pdf/2203.15556">Read Paper</a></p>
<h3
id="instructgpt-training-language-models-to-follow-instructions">InstructGPT:
Training Language Models to Follow Instructions</h3>
<p>Ouyang et al. 2022<br />
InstructGPT fine-tuned GPT-3 using human feedback to improve alignment
with user intentions, marking a significant step towards safer and more
user-friendly LLMs.<br />
<a href="https://arxiv.org/pdf/2203.02155">Read Paper</a></p>
